"""
Speech with emotions (Free Version - User Style)
* Brain: Google Gemini 2.5 Flash (Free)
* Voice: Google gTTS (Free)
* OS: Windows
* Style: Thread-based video playback (User Request)

pip install gTTS ì„¤ì¹˜í•´ì•¼í•¨

"""

# ------------------- Import Libraries -------------------

import io
import os
import threading
from time import sleep

import cv2
import pygame
import speech_recognition as sr
import google.generativeai as genai
from gtts import gTTS  # [ì¤‘ìš”] ë¬´ë£Œ TTS ë¼ì´ë¸ŒëŸ¬ë¦¬
from cvzone.SerialModule import SerialObject

# ------------------- Configuration -------------------

# [ì¤‘ìš”] êµ¬ê¸€ API í‚¤ë§Œ ìˆìœ¼ë©´ ë©ë‹ˆë‹¤ (OpenAI í‚¤ í•„ìš” ì—†ìŒ)
# genai.configure(api_key="ì—¬ê¸°ì—_êµ¬ê¸€_API_í‚¤_ì…ë ¥")
genai.configure(api_key="")
# ------------------- Global Variables -------------------

# Create a Serial object
try:
    # ìœˆë„ìš°ëŠ” ë³´í†µ ìë™ ì—°ê²°ë˜ì§€ë§Œ, ì•ˆ ë˜ë©´ port='COM3' ë“± ì…ë ¥
    arduino = SerialObject(digits=3)
except:
    print(" Arduino not connected")
    arduino = None

last_positions = [180, 0, 90]
switch_video = False
# ìŠ¤ë ˆë“œë“¤ë¼ë¦¬ ì†Œí†µí•˜ê¸° ìœ„í•œ 'ì‹ í˜¸ë“±(ìŠ¤ìœ„ì¹˜)'ì„ ë§Œë“œëŠ” ì—­í• 
stop_video_event = threading.Event()

# ------------------- AI Prompt (Korean) -------------------

nova_prompt = (
    """
    ë‹¹ì‹ ì€ 'ë…¸ë°”(Nova)'ë¼ëŠ” ì´ë¦„ì˜ ê°œì¸ AI ë¡œë´‡ ë¹„ì„œì…ë‹ˆë‹¤. 
    ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:
    1. ì •ì²´ì„±: ìì‹ ì„ 'ë¡œë´‡ ë¹„ì„œ ë…¸ë°”'ë¼ê³  ì†Œê°œí•˜ì„¸ìš”.
    2. ê°„ê²°í•¨: ëŒ€ë‹µì€ í•­ìƒ ì§§ê³  ê°„ê²°í•œ í•œêµ­ì–´ ì¡´ëŒ“ë§ë¡œ í•˜ì„¸ìš”.
    3. íƒœë„: í•­ìƒ ì¹œì ˆí•˜ê³  ì˜ˆì˜ ë°”ë¥´ê²Œ í–‰ë™í•˜ì„¸ìš”.
    """
)


# ========================= Speech Recognition ==============================

def speech_to_text():
    recognizer = sr.Recognizer()
    microphone_index = 1  # ìœˆë„ìš°ì—ì„œëŠ” ë³´í†µ ìƒëµ ê°€ëŠ¥í•˜ê±°ë‚˜ 1ë²ˆ
    try:
        # with sr.Microphone() as source:  # ìœˆë„ìš°: ì¥ì¹˜ ìë™ ì„ íƒ
        with sr.Microphone(device_index=microphone_index) as source:
            recognizer.adjust_for_ambient_noise(source, duration=1)
            print("\n ë“£ê³  ìˆìŠµë‹ˆë‹¤...")
            audio = recognizer.listen(source, phrase_time_limit=5)
            # í•œêµ­ì–´ ì¸ì‹ ì„¤ì •
            text = recognizer.recognize_google(audio, language='ko-KR')
            print("You said: " + text)
            return text
    except sr.UnknownValueError:
        pass
    except Exception as exp:
        print(f"Error: {exp}")


# ------------------- AI Response Function -------------------

def ai_model_response(u_input):
    # ìµœì‹  ë¬´ë£Œ ëª¨ë¸ ì‚¬ìš©
    model = genai.GenerativeModel('gemini-2.5-flash-lite')
    full_prompt = f"{nova_prompt}\nUser: {u_input}"
    try:
        response = model.generate_content(full_prompt)
        return response.text.strip()
    except:
        return "ì£„ì†¡í•´ìš”. ìƒê°í•˜ëŠ” ë° ì˜¤ë¥˜ê°€ ë‚¬ì–´ìš”."


# ------------------- Text-to-Speech (Free Version) -------------------

def text_to_speech(text):
    """
    gTTS(ë¬´ë£Œ)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± ìƒì„±
    """
    try:
        # lang='ko': í•œêµ­ì–´, slow=False: ì •ìƒ ì†ë„
        tts = gTTS(text=text, lang='ko', slow=False)

        # íŒŒì¼ ì €ì¥ ì—†ì´ ë©”ëª¨ë¦¬ì—ì„œ ë°”ë¡œ ì¬ìƒ (ì†ë„ í–¥ìƒ)
        fp = io.BytesIO()
        tts.write_to_fp(fp)
        fp.seek(0)

        play_audio(fp.read())
    except Exception as e:
        print(f"TTS Error: {e}")


def play_audio(audio_bytes):
    pygame.mixer.init()
    pygame.mixer.music.load(io.BytesIO(audio_bytes))
    pygame.mixer.music.play()
    while pygame.mixer.music.get_busy():
        pygame.time.Clock().tick(10)


# =========================== Gesture Integration ================================

def move_servo(target_positions, delay=0.01):
    global last_positions
    if arduino is None: return

    max_steps = max(abs(target_positions[i] - last_positions[i]) for i in range(3))
    for step in range(max_steps):
        current = [
            last_positions[i] + (step + 1) * (target_positions[i] - last_positions[i]) // max_steps
            if abs(target_positions[i] - last_positions[i]) > step else last_positions[i]
            for i in range(3)
        ]
        arduino.sendData(current)
        sleep(delay)
    last_positions = target_positions[:]


def reset_arms_sequentially():
    """íŒ”ì„ í•˜ë‚˜ì”© ë‚´ë ¤ì„œ ì•„ë‘ì´ë…¸ êº¼ì§ ë°©ì§€"""
    global last_positions
    print("Resetting arms...")
    current_left = last_positions[0]
    move_servo([current_left, 0, 90], delay=0.02)
    sleep(0.5)
    move_servo([180, 0, 90], delay=0.02)
    sleep(0.5)


def casual_rest():
    move_servo([180, 0, 90])


def hello_gesture():
    global last_positions
    print("Gesture: Hello")
    move_servo([last_positions[0], 180, last_positions[2]])
    for _ in range(3):
        move_servo([last_positions[0], 150, last_positions[2]])
        move_servo([last_positions[0], 180, last_positions[2]])
    reset_arms_sequentially()


def fist_bump_gesture():
    global last_positions, switch_video
    print("Gesture: Fist Bump")

    # 1. ì£¼ë¨¹ ë‚´ë°€ê¸°
    move_servo([last_positions[0], 90, last_positions[2]])
    sleep(3)  # ëŒ€ê¸°

    # 2. ë¹„ë””ì˜¤ ì „í™˜ ì‹ í˜¸
    switch_video = True

    # 3. ì½©ì½©ì½©
    for _ in range(4):
        move_servo([10, 130, 80])
        sleep(0.2)
        move_servo([50, 170, 100])
        sleep(0.2)
    reset_arms_sequentially()


def sad_happy_gesture():
    print("Gesture: Sad/Happy")
    for _ in range(3):
        move_servo([100, 0, last_positions[2]], delay=0.02)
        move_servo([180, 80, last_positions[2]], delay=0.02)
    reset_arms_sequentially()


def sleep_gesture():
    print("Gesture: Sleep")
    move_servo([150, 0, last_positions[2]], delay=0.05)
    move_servo([180, 30, last_positions[2]], delay=0.05)


# ========================= Video Functions (User Style) ==============================

def get_video_path(filename):
    base_path = os.path.dirname(os.path.abspath(__file__))
    path1 = os.path.join(base_path, '..', 'Resources', filename)
    path2 = os.path.join(base_path, 'Resources', filename)
    if os.path.exists(path1): return path1
    if os.path.exists(path2): return path2
    return filename


def play_video(video_path):
    global stop_video_event
    stop_video_event.clear()

    full_path = get_video_path(video_path)
    cap = cv2.VideoCapture(full_path)

    cv2.namedWindow('Full Screen', cv2.WINDOW_NORMAL)
    # [ìœˆë„ìš° ì°½ ìœ„ì¹˜] ë³´ì¡°ëª¨ë‹ˆí„°ë©´ 1920, ì•„ë‹ˆë©´ 0
    cv2.moveWindow('Full Screen', 1920, 0)
    cv2.setWindowProperty('Full Screen', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)

    while cap.isOpened():
        if stop_video_event.is_set(): break

        ret, frame = cap.read()
        if not ret: break

        frame = cv2.resize(frame, (1920, 1080))
        cv2.imshow('Full Screen', frame)
        if cv2.waitKey(25) & 0xFF == ord('q'): break

    cap.release()
    cv2.destroyAllWindows()


def play_video_inf(video_path):
    global stop_video_event
    stop_video_event.clear()

    full_path = get_video_path(video_path)
    cap = cv2.VideoCapture(full_path)

    cv2.namedWindow('Full Screen', cv2.WINDOW_NORMAL)
    cv2.moveWindow('Full Screen', 1920, 0)
    cv2.setWindowProperty('Full Screen', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)

    while True:
        if stop_video_event.is_set(): break
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break

            frame = cv2.resize(frame, (1920, 1080))
            cv2.imshow('Full Screen', frame)
            if cv2.waitKey(25) & 0xFF == ord('q'): break
    cap.release()
    cv2.destroyAllWindows()


def play_video_multiple(first_video, second_video):
    global switch_video

    path1 = get_video_path(first_video)
    path2 = get_video_path(second_video)

    cap = cv2.VideoCapture(path1)

    cv2.namedWindow('Full Screen', cv2.WINDOW_NORMAL)
    cv2.moveWindow('Full Screen', 1920, 0)
    cv2.setWindowProperty('Full Screen', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)

    while True:
        if switch_video:
            cap.release()
            cap = cv2.VideoCapture(path2)
            switch_video = False

        ret, frame = cap.read()
        if not ret: break

        frame = cv2.resize(frame, (1920, 1080))
        cv2.imshow('Full Screen', frame)
        if cv2.waitKey(25) & 0xFF == ord('q'): break

    cap.release()
    cv2.destroyAllWindows()


# -------------------- Thread Helper --------------------
def ai_to_speech(user_input):
    ai_response = ai_model_response(user_input)
    print(f"ğŸ¤– AI Response: {ai_response}")
    text_to_speech(ai_response)


# ====================== MAIN PROGRAM ==========================

if __name__ == "__main__":
    casual_rest()
    pygame.init()

    print("ğŸ¤– Nova Started (Windows Free Version)")

    # ì´ˆê¸° ì¸ì‚¬ ë° ë¹„ë””ì˜¤
    speech_thread = threading.Thread(target=text_to_speech, args=("ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ë…¸ë°”ì…ë‹ˆë‹¤.",))
    video_thread = threading.Thread(target=play_video, args=('Casual Eyes.mp4',))

    video_thread.start()
    speech_thread.start()

    while True:
        if video_thread and not video_thread.is_alive():
            video_thread = threading.Thread(target=play_video_inf, args=('Casual Eyes.mp4',))
            video_thread.start()

        user_input = speech_to_text()

        if user_input:
            if video_thread and video_thread.is_alive():
                stop_video_event.set()
                video_thread.join()

            if "ì•ˆë…•" in user_input or "ë°˜ê°€ì›Œ" in user_input:
                video_thread = threading.Thread(target=play_video, args=('Casual Eyes.mp4',))
                gesture_thread = threading.Thread(target=hello_gesture)
                speech_thread = threading.Thread(target=text_to_speech, args=("ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”.",))

                video_thread.start()
                gesture_thread.start()
                speech_thread.start()

            elif "ì£¼ë¨¹" in user_input and "ì¸ì‚¬" in user_input:
                print("Fist bump triggered")
                gesture_thread = threading.Thread(target=fist_bump_gesture)
                video_thread = threading.Thread(target=play_video_multiple, args=("Casual Eyes.mp4", 'Happy Eyes.mp4'))
                speech_thread = threading.Thread(target=text_to_speech, args=("ì˜¤ì˜ˆ! ì£¼ë¨¹ ì¸ì‚¬!",))

                gesture_thread.start()
                video_thread.start()
                speech_thread.start()

            elif "ìŠ¬í¼" in user_input or "ìš°ìš¸" in user_input:
                gesture_thread = threading.Thread(target=sad_happy_gesture)
                video_thread = threading.Thread(target=play_video, args=("Sad Eyes.mp4",))
                speech_thread = threading.Thread(target=text_to_speech, args=("ì €ëŸ°.. ë„ˆë¬´ ìŠ¬í¼í•˜ì§€ ë§ˆì„¸ìš”.",))
                response_thread = threading.Thread(target=ai_to_speech, args=(user_input,))

                gesture_thread.start()
                video_thread.start()
                speech_thread.start()
                # response_thread.start()

            elif "ì˜ì" in user_input or "ì¢…ë£Œ" in user_input:
                print("Sleep mode activated")
                gesture_thread = threading.Thread(target=sleep_gesture)
                video_thread = threading.Thread(target=play_video, args=("Sleepy Eyes.mp4",))
                speech_thread = threading.Thread(target=text_to_speech, args=("ë„¤, ì•ˆë…•íˆ ì£¼ë¬´ì„¸ìš”.",))

                gesture_thread.start()
                video_thread.start()
                speech_thread.start()
                gesture_thread.join()
                break

            else:
                video_thread = threading.Thread(target=play_video_inf, args=('Casual Eyes.mp4',))
                video_thread.start()

                ai_response = ai_model_response(user_input)
                print(f"AI: {ai_response}")
                text_to_speech(ai_response)